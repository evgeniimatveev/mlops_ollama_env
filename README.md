# ğŸ§  FastAPI + Ollama Playground â€” Dockerized LLM Inference

![FastAPI](https://img.shields.io/badge/FastAPI-Backend-009688?logo=fastapi&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?logo=docker&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?logo=githubactions&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

Minimal **FastAPI** service that connects to **Ollama** and ships with a tiny web **playground** (model selector, temperature, top-p, max tokens, streaming).  
Runs with one command via **Docker Compose**.

---

## ğŸ“¦ Tech Stack
- **FastAPI + Uvicorn** â€” API & simple HTML playground  
- **Ollama** â€” local LLMs (phi3, qwen2.5-coder, llama3, deepseek-r1, â€¦)  
- **Docker / Compose** â€” reproducible environment  
- **dotenv** â€” config via `.env`  
- **pytest** â€” optional tests (if you add them)  
- **GitHub Actions** â€” optional CI/CD for container images

---

## âœ¨ Features
- `GET /models` â€” proxies Ollama `/api/tags` (model list)  
- `GET/POST /chat` â€” non-stream response  
- `GET /stream` â€” token streaming (text/plain)  
- `GET /playground` â€” minimal web UI  
- `GET /health` â€” connectivity check  
- (Optional) `GET /warmup?model=phi3:mini` â€” preload/keep model warm

---

## ğŸ—‚ï¸ Project Structure
```bash
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ .env                # local (do NOT commit)
â”œâ”€â”€ .env.example        # template to share
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

âš™ï¸ Environment Setup

Copy .env.example â†’ .env and adjust as needed.

# If Ollama runs on the host (Windows/macOS):
OLLAMA_URL=http://host.docker.internal:11434

# Linux: either add extra_hosts in compose or use the docker bridge IP (e.g.):
# OLLAMA_URL=http://172.17.0.1:11434

DEFAULT_MODEL=phi3:mini
ALLOWED_MODELS=phi3:mini,qwen2.5-coder:7b,llama3.1:latest,deepseek-r1:7b

# Optional tuning
READ_TIMEOUT=300
OLLAMA_KEEP_ALIVE=30m

ğŸš€ Quick Start
A) Using Your Hostâ€™s Ollama

Start Ollama and pull at least one model:

ollama serve
ollama pull phi3:mini


Then build and run:

docker compose up -d --build


Access endpoints:

ğŸ¨ Playground â†’ http://127.0.0.1:8000/playground

ğŸ“˜ Docs â†’ http://127.0.0.1:8000/docs

ğŸ’š Health â†’ http://127.0.0.1:8000/health

B) Self-contained (Compose Runs Ollama Too)

Use this version of docker-compose.yml:

version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  ollama_api:
    build: .
    container_name: ollama_api
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:


Then run:

docker compose up -d --build

ğŸ”Œ Endpoints (Cheat Sheet)
Method	Endpoint	Description
GET	/models	List available models
GET / POST	/chat	Generate text
GET	/stream	Stream responses
GET	/playground	UI
GET	/health	Status check
GET	/warmup?model=phi3:mini	Warm up model

Example JSON for /chat:

{
  "prompt": "hello",
  "model": "phi3:mini",
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 256
}

ğŸ³ Docker Setup
Standard Compose (Uses Host Ollama)
version: "3.9"
services:
  ollama_api:
    build: .
    container_name: ollama_api
    ports:
      - "8000:8000"
    env_file:
      - .env
    restart: unless-stopped

Compose with Ollama Service (Self-contained)
version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  ollama_api:
    build: .
    container_name: ollama_api
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:

âš¡ Performance Tips

Use /stream for faster perceived output

Warm up models via GET /warmup?model=phi3:mini

Lightweight models (phi3:mini, qwen2.5-coder:7b) run faster than large ones

ğŸ§° Troubleshooting

502 from /health â€” Check if OLLAMA_URL is reachable:

curl http://<host>:11434/api/tags


Linux fix â€” Add to docker-compose.yml:

extra_hosts:
  - "host.docker.internal:host-gateway"


Keep OLLAMA_URL=http://host.docker.internal:11434.

No models listed? â†’ Run:

ollama pull phi3:mini

ğŸ” CI/CD (GitHub Container Registry)

.github/workflows/docker.yml:

name: Docker CI/CD
on:
  push:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build_and_push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - uses: docker/metadata-action@v5
        id: meta
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha
      - uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}


Resulting image:

ghcr.io/<owner>/<repo>:main
ghcr.io/<owner>/<repo>:sha-...

ğŸ§¾ .dockerignore
__pycache__/
*.pyc
*.log
.venv/
.env
.git
.gitignore
tests/

ğŸªª LICENSE (MIT)
MIT License

Copyright (c) 2025 Evgenii Matveev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
[standard MIT text continues...]

âœ… Quick Push Checklist

 README.md added

 .env.example included (real .env excluded)

 .dockerignore present

 (optional) LICENSE added

 (optional) .github/workflows/docker.yml for CI/CD

ğŸ‘¤ Author

Evgenii Matveev
ğŸŒ GitHub

ğŸ’¼ LinkedIn

ğŸ“Š Portfolio


---

âœ… Everything is in **one clean file**: README + `.env.example` + `.dockerignore` + MIT License + CI/CD + checklist.  
Perfect for a professional GitHub release.

Would you like me to add a **preview image** block at the top (like a screenshot of your playground UI)? Itâ€™ll make the repo look even more polished.

You said:
# ğŸ§  FastAPI + Ollama Playground â€” Dockerized LLM Inference
