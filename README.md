# ğŸ§  FastAPI + Ollama Playground â€” Dockerized LLM Inference

![FastAPI](https://img.shields.io/badge/FastAPI-Backend-009688?logo=fastapi&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?logo=docker&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?logo=githubactions&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

Minimal **FastAPI** service that connects to **Ollama** and ships with a tiny web **playground** (model selector, temperature, top-p, max tokens, streaming).  
Runs with one command via **Docker Compose**.

---

## ğŸ“¦ Tech Stack
- **FastAPI + Uvicorn** â€” API & simple HTML playground  
- **Ollama** â€” local LLMs (phi3, qwen2.5-coder, llama3, deepseek-r1, â€¦)  
- **Docker / Compose** â€” reproducible environment  
- **dotenv** â€” config via `.env`  
- **pytest** â€” optional tests (if you add them)  
- **GitHub Actions** â€” optional CI/CD for container images

---

## âœ¨ Features
- `GET /models` â€” proxies Ollama `/api/tags` (model list)  
- `GET/POST /chat` â€” non-stream response  
- `GET /stream` â€” token streaming (text/plain)  
- `GET /playground` â€” minimal web UI  
- `GET /health` â€” connectivity check  
- (Optional) `GET /warmup?model=phi3:mini` â€” preload/keep model warm

---

## ğŸ—‚ï¸ Project Structure
```bash
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ .env                # local (do NOT commit)
â”œâ”€â”€ .env.example        # template to share
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```


## ğŸ“¢ Stay Connected!  
ğŸ’» **GitHub Repository:** [Evgenii Matveev](https://github.com/evgeniimatveev)  
ğŸŒ **Portfolio:** [Data Science Portfolio](https://www.datascienceportfol.io/evgeniimatveevusa)  
ğŸ“Œ **LinkedIn:** [Evgenii Matveev](https://www.linkedin.com/in/evgenii-matveev-510926276/) 
