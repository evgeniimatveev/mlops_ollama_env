# ğŸ§  FastAPI + Ollama Playground â€” Dockerized LLM Inference

![FastAPI](https://img.shields.io/badge/FastAPI-Backend-009688?logo=fastapi&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?logo=docker&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?logo=githubactions&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

Minimal **FastAPI** service that connects to **Ollama** and ships with a tiny web **playground** (model selector, temperature, top-p, max tokens, streaming).  
Runs with one command via **Docker Compose**.

---

## ğŸ“¦ Tech Stack
- **FastAPI + Uvicorn** â€” API & simple HTML playground  
- **Ollama** â€” local LLMs (phi3, qwen2.5-coder, llama3, deepseek-r1, â€¦)  
- **Docker / Compose** â€” reproducible environment  
- **dotenv** â€” config via `.env`  
- **pytest** â€” optional tests (if you add them)  
- **GitHub Actions** â€” optional CI/CD for container images  

---

## âœ¨ Features
- `GET /models` â€” proxies Ollama `/api/tags` (model list)  
- `GET/POST /chat` â€” non-stream response  
- `GET /stream` â€” token streaming (text/plain)  
- `GET /playground` â€” minimal web UI  
- `GET /health` â€” connectivity check  
- (Optional) `GET /warmup?model=phi3:mini` â€” preload/keep model warm  

---

## ğŸ—‚ï¸ Project Structure
```bash
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ .env                # local (do NOT commit)
â”œâ”€â”€ .env.example        # template to share
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup
Copy `.env.example` â†’ `.env` and adjust as needed.

```env
# If Ollama runs on the host (Windows/macOS)
OLLAMA_URL=http://host.docker.internal:11434

# Linux: either add extra_hosts in compose or use the docker bridge IP
# OLLAMA_URL=http://172.17.0.1:11434

DEFAULT_MODEL=phi3:mini
ALLOWED_MODELS=phi3:mini,qwen2.5-coder:7b,llama3.1:latest,deepseek-r1:7b

# Optional tuning
READ_TIMEOUT=300
OLLAMA_KEEP_ALIVE=30m
```

---

## ğŸš€ Quick Start

### ğŸ§© A) Using Your Hostâ€™s Ollama

Start Ollama and pull at least one model:

```bash
ollama serve
ollama pull phi3:mini
```

Then build and run:

```bash
docker compose up -d --build
```

Access endpoints:

- ğŸ¨ Playground â†’ http://127.0.0.1:8000/playground  
- ğŸ“˜ Docs â†’ http://127.0.0.1:8000/docs  
- ğŸ’š Health â†’ http://127.0.0.1:8000/health  

---

### ğŸ³ B) Self-contained (Compose Runs Ollama Too)

If you want both Ollama + API in Docker:

```yaml
version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  ollama_api:
    build: .
    container_name: ollama_api
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
```

Run everything:

```bash
docker compose up -d --build
```

---

### ğŸ”Œ Endpoints (Cheat Sheet)

| Method | Endpoint | Description |
|--------|-----------|--------------|
| GET | `/models` | List available models |
| GET / POST | `/chat` | Generate completions |
| GET | `/stream` | Stream tokens in real-time |
| GET | `/playground` | Web UI |
| GET | `/health` | Health check |
| GET | `/warmup?model=phi3:mini` | Warm up model |

Example JSON for `/chat`:

```json
{
  "prompt": "hello",
  "model": "phi3:mini",
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 256
}
```

---

### âš¡ Performance Tips
- Use `/stream` for faster perceived responses  
- Warm up models with `GET /warmup?model=phi3:mini`  
- Lightweight models (`phi3:mini`, `qwen2.5-coder:7b`) respond faster  

---

### ğŸ§° Troubleshooting

**502 from /health â€” verify OLLAMA_URL:**

```bash
curl http://<host>:11434/api/tags
```

**Linux host fix:**

Add in `docker-compose.yml`:

```yaml
extra_hosts:
  - "host.docker.internal:host-gateway"
```

Then keep:

```env
OLLAMA_URL=http://host.docker.internal:11434
```

**No models listed?** â€” run:

```bash
ollama pull phi3:mini
```

---

### ğŸ” CI/CD (Optional: GitHub Container Registry)

Create `.github/workflows/docker.yml`:

```yaml
name: Docker CI/CD
on:
  push:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build_and_push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - uses: docker/metadata-action@v5
        id: meta
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha
      - uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
```

Image examples:

```
ghcr.io/<owner>/<repo>:main  
ghcr.io/<owner>/<repo>:sha-...
```

---

### ğŸ§¾ .dockerignore

```
__pycache__/
*.pyc
*.log
.venv/
.env
.git
.gitignore
tests/
```

---

### ğŸªª License (MIT)

MIT License  

Copyright (c) 2025 Evgenii Matveev  

Permission is hereby granted, free of charge, to any person obtaining a copy  
of this software and associated documentation files (the "Software"), to deal  
in the Software without restriction, including without limitation the rights  
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell  
copies of the Software, and to permit persons to whom the Software is  
furnished to do so, subject to the following conditions:  

[standard MIT text continues...]

---

### âœ… Quick Push Checklist
- README.md complete  
- .env.example included  
- .dockerignore added  
- LICENSE added  
- Optional CI/CD workflow ready  


# ğŸ“¢ Stay Connected!  
ğŸ’» **GitHub Repository:** [Evgenii Matveev](https://github.com/evgeniimatveev)  
ğŸŒ **Portfolio:** [Data Science Portfolio](https://www.datascienceportfol.io/evgeniimatveevusa)  
ğŸ“Œ **LinkedIn:** [Evgenii Matveev](https://www.linkedin.com/in/evgenii-matveev-510926276/)  
